{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#erudito-easy-apicli-to-ask-questions-about-your-documentation","title":"ERUDITO: Easy API/CLI to ask questions about your documentation","text":"<p>FastAPI/Typer application that uses LlamaCpp and GPT4All to answer questions about your data. Everything runs locally.</p>   Logo generated by Stable Diffusion V2"},{"location":"faiss_wrapper/","title":"Faiss wrapper","text":""},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex","title":"<code>FaissIndex</code>","text":"<p>A class for creating and querying a Faiss index.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>faiss.Index</code> <p>The Faiss index object used for similarity search.</p> <code>reverse_index</code> <code>dict</code> <p>A dictionary mapping document IDs to their corresponding index in the Faiss index. This allows for quick lookup of document vectors during query time.</p> Source code in <code>src/utils/Faiss.py</code> <pre><code>class FaissIndex:\n\"\"\"\n    A class for creating and querying a Faiss index.\n    Attributes:\n        index (faiss.Index): The Faiss index object used for similarity search.\n        reverse_index (dict): A dictionary mapping document IDs to their corresponding\n            index in the Faiss index. This allows for quick lookup of document vectors\n            during query time.\n    \"\"\"\ndef __init__(\nself,\n):\n\"\"\"\n        Initializes an empty Faiss index and an empty reverse index.\n        \"\"\"\nself.index = None\nself.reverse_index = {}\ndef load(self, index_file: Path) -&gt; None:\n\"\"\"\n        Load the Faiss index and its corresponding reverse index from disk.\n        Args:\n            index_file (Path): The path to the Faiss index file.\n        Raises:\n            ValueError: If the index file or reverse index file is not found.\n        \"\"\"\nreverse_index_file = index_file.parent / \"reverse_index.npy\"\nif not index_file.exists():\nraise ValueError(f\"No index found in {index_file}\")\nelif not reverse_index_file.exists():\nraise ValueError(f\"No reverse index found in {self.reverse_index}\")\nelse:\nself.index = faiss.read_index(str(index_file))\nself.reverse_index = np.load(\nstr(reverse_index_file), allow_pickle=True\n).item()\ndef save(self, index_file: Path) -&gt; None:\n\"\"\"\n        Save the Faiss index to disk.\n        Args:\n            index_file (Path): The path to the Faiss index file.\n        \"\"\"\nfaiss.write_index(self.index, str(index_file))\nreverse_index_file = index_file.parent / \"reverse_index.npy\"\nnp.save(str(reverse_index_file), self.reverse_index)\ndef add_vectors(self, vectors: np.ndarray, contents: List[str]) -&gt; None:\n\"\"\"\n        Add vectors and their corresponding contents to the Faiss index.\n        Args:\n            vectors (np.ndarray): The vectors to add to the index. Shape should be (num_vectors, vector_dim).\n            contents (List[str]): The corresponding contents for each vector. Length should be num_vectors.\n        Raises:\n            ValueError: If the length of contents does not match the number of vectors.\n        \"\"\"\nif not self.index:\nself.index = faiss.IndexIDMap(faiss.IndexFlatL2(vectors.shape[1]))\nnum_added = 0\nelse:\nnum_added = self.index.ntotal\nnum_vectors = vectors.shape[0]\nids = np.arange(num_added, num_added + num_vectors)\nself.index.add_with_ids(vectors, ids)\nfor i in range(num_vectors):\nself.reverse_index[ids[i]] = contents[i]\ndef search(self, query_embedding: np.ndarray) -&gt; List[str]:\n\"\"\"\n        Search the Faiss index for the nearest neighbors of a given query embedding.\n        Args:\n            query_embedding (np.ndarray): An array of shape (1, D) containing the query embedding,\n                where D is the dimensionality of the embeddings used to build the index.\n        Returns:\n            A list of strings, each of which corresponds to the content of the document that\n            is closest to the query embedding in the embedding space.\n        Raises:\n            AssertionError: If the index has not been loaded yet.\n            AssertionError: If the dimensionality of the query embedding does not match that of the index.\n        \"\"\"\nassert self.index is not None, \"Index has not been loaded yet\"\nassert (\nquery_embedding.shape[1] == self.index.d\n), \"Embedding dimensions do not match\"\ndistances, indices = self.index.search(query_embedding, 1)\nresult_ids = [self.reverse_index[int(i)] for i in indices]\nreturn result_ids\n</code></pre>"},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex.__init__","title":"<code>__init__()</code>","text":"<p>Initializes an empty Faiss index and an empty reverse index.</p> Source code in <code>src/utils/Faiss.py</code> <pre><code>def __init__(\nself,\n):\n\"\"\"\n    Initializes an empty Faiss index and an empty reverse index.\n    \"\"\"\nself.index = None\nself.reverse_index = {}\n</code></pre>"},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex.add_vectors","title":"<code>add_vectors(vectors, contents)</code>","text":"<p>Add vectors and their corresponding contents to the Faiss index.</p> <p>Parameters:</p> Name Type Description Default <code>vectors</code> <code>np.ndarray</code> <p>The vectors to add to the index. Shape should be (num_vectors, vector_dim).</p> required <code>contents</code> <code>List[str]</code> <p>The corresponding contents for each vector. Length should be num_vectors.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of contents does not match the number of vectors.</p> Source code in <code>src/utils/Faiss.py</code> <pre><code>def add_vectors(self, vectors: np.ndarray, contents: List[str]) -&gt; None:\n\"\"\"\n    Add vectors and their corresponding contents to the Faiss index.\n    Args:\n        vectors (np.ndarray): The vectors to add to the index. Shape should be (num_vectors, vector_dim).\n        contents (List[str]): The corresponding contents for each vector. Length should be num_vectors.\n    Raises:\n        ValueError: If the length of contents does not match the number of vectors.\n    \"\"\"\nif not self.index:\nself.index = faiss.IndexIDMap(faiss.IndexFlatL2(vectors.shape[1]))\nnum_added = 0\nelse:\nnum_added = self.index.ntotal\nnum_vectors = vectors.shape[0]\nids = np.arange(num_added, num_added + num_vectors)\nself.index.add_with_ids(vectors, ids)\nfor i in range(num_vectors):\nself.reverse_index[ids[i]] = contents[i]\n</code></pre>"},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex.load","title":"<code>load(index_file)</code>","text":"<p>Load the Faiss index and its corresponding reverse index from disk.</p> <p>Parameters:</p> Name Type Description Default <code>index_file</code> <code>Path</code> <p>The path to the Faiss index file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the index file or reverse index file is not found.</p> Source code in <code>src/utils/Faiss.py</code> <pre><code>def load(self, index_file: Path) -&gt; None:\n\"\"\"\n    Load the Faiss index and its corresponding reverse index from disk.\n    Args:\n        index_file (Path): The path to the Faiss index file.\n    Raises:\n        ValueError: If the index file or reverse index file is not found.\n    \"\"\"\nreverse_index_file = index_file.parent / \"reverse_index.npy\"\nif not index_file.exists():\nraise ValueError(f\"No index found in {index_file}\")\nelif not reverse_index_file.exists():\nraise ValueError(f\"No reverse index found in {self.reverse_index}\")\nelse:\nself.index = faiss.read_index(str(index_file))\nself.reverse_index = np.load(\nstr(reverse_index_file), allow_pickle=True\n).item()\n</code></pre>"},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex.save","title":"<code>save(index_file)</code>","text":"<p>Save the Faiss index to disk.</p> <p>Parameters:</p> Name Type Description Default <code>index_file</code> <code>Path</code> <p>The path to the Faiss index file.</p> required Source code in <code>src/utils/Faiss.py</code> <pre><code>def save(self, index_file: Path) -&gt; None:\n\"\"\"\n    Save the Faiss index to disk.\n    Args:\n        index_file (Path): The path to the Faiss index file.\n    \"\"\"\nfaiss.write_index(self.index, str(index_file))\nreverse_index_file = index_file.parent / \"reverse_index.npy\"\nnp.save(str(reverse_index_file), self.reverse_index)\n</code></pre>"},{"location":"faiss_wrapper/#src.utils.Faiss.FaissIndex.search","title":"<code>search(query_embedding)</code>","text":"<p>Search the Faiss index for the nearest neighbors of a given query embedding.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>np.ndarray</code> <p>An array of shape (1, D) containing the query embedding, where D is the dimensionality of the embeddings used to build the index.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, each of which corresponds to the content of the document that</p> <code>List[str]</code> <p>is closest to the query embedding in the embedding space.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the index has not been loaded yet.</p> <code>AssertionError</code> <p>If the dimensionality of the query embedding does not match that of the index.</p> Source code in <code>src/utils/Faiss.py</code> <pre><code>def search(self, query_embedding: np.ndarray) -&gt; List[str]:\n\"\"\"\n    Search the Faiss index for the nearest neighbors of a given query embedding.\n    Args:\n        query_embedding (np.ndarray): An array of shape (1, D) containing the query embedding,\n            where D is the dimensionality of the embeddings used to build the index.\n    Returns:\n        A list of strings, each of which corresponds to the content of the document that\n        is closest to the query embedding in the embedding space.\n    Raises:\n        AssertionError: If the index has not been loaded yet.\n        AssertionError: If the dimensionality of the query embedding does not match that of the index.\n    \"\"\"\nassert self.index is not None, \"Index has not been loaded yet\"\nassert (\nquery_embedding.shape[1] == self.index.d\n), \"Embedding dimensions do not match\"\ndistances, indices = self.index.search(query_embedding, 1)\nresult_ids = [self.reverse_index[int(i)] for i in indices]\nreturn result_ids\n</code></pre>"},{"location":"ingest/","title":"Ingest","text":"<p>This module provides functionality for ingesting text files and creating a vector store for them using Llama embeddings and Faiss indexing.</p> Note <p>The vector store is created using Faiss indexing, and it is saved in a new folder in the 'index' directory. The name of the new folder is the same as the name of the folder containing the text documents.</p>"},{"location":"ingest/#src.ingest.create_store","title":"<code>create_store(chunks, folder_name='vector_store')</code>","text":"<p>Create the vector store for given text chunks using model embeddings and Faiss indexing.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <p>List[str] : Required : A list of text chunks to create the vector store from.</p> required <code>folder_name</code> <p>str : Optional : The name of the folder where the vector store will be saved. Default is \"vector_store\".</p> <code>'vector_store'</code> Source code in <code>src/ingest.py</code> <pre><code>def create_store(chunks: List[str], folder_name: str = \"vector_store\"):\n\"\"\"\n    Create the vector store for given text chunks using model embeddings and Faiss indexing.\n    Args:\n        chunks : List[str] : Required : A list of text chunks to create the vector store from.\n        folder_name : str : Optional : The name of the folder where the vector store will be saved. Default is \"vector_store\".\n    \"\"\"\nindex = FaissIndex()\nembeddings = np.empty((len(chunks), 4096))\nindex_path = Path(\"index\") / folder_name / \"index.faiss\"\n# check if the index already exists\nif index_path.exists():\nlogger.info(\"\ud83e\uddbe Updating existing index. This could take a while...\")\nindex.load(index_path)\nelse:\nindex_path.parent.mkdir(parents=True, exist_ok=True)\nlogger.info(\"\ud83d\ude9d Create vector store. This could take a while...\")\ncount = 0\nfor i in track(range(len(chunks)), description=\"Embedding \ud83e\udd96\"):\ntry:\nembeddings[i] = llama.embed(chunks[i])\ncount += 1\nexcept KeyboardInterrupt:\nlogger.warning(f\"\u274c Stopped at {count} out of {len(chunks)}\")\nindex.add_vectors(embeddings, chunks)\nindex.save(index_path)\nbreak\n# add the embeddings to the vector store\nindex.add_vectors(embeddings, chunks)\nindex.save(index_path)\nlogger.info(f\"\ud83e\udd52 Save FAISS vector store into a pickle in index/{folder_name}\")\n</code></pre>"},{"location":"ingest/#src.ingest.ingest","title":"<code>ingest(documentation_path=typer.Argument(Ellipsis, help='Folder containing the documents.'), model_path=typer.Argument(Ellipsis, help='Folder containing the model.'))</code>","text":"<p>Ingest all the text files from <code>documentation_path</code> and create a vector store using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>documentation_path</code> <code>str</code> <p>Path to the folder containing the text documents to be ingested.</p> <code>typer.Argument(Ellipsis, help='Folder containing the documents.')</code> <code>model_path</code> <code>str</code> <p>Path to the folder containing the Llama model.</p> <code>typer.Argument(Ellipsis, help='Folder containing the model.')</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If no documents are found inside the <code>documentation_path</code> folder.</p> Notes <p>This function creates a new folder with the embedding index in the <code>index</code> directory.</p> Source code in <code>src/ingest.py</code> <pre><code>def ingest(\ndocumentation_path: str = typer.Argument(\n..., help=\"Folder containing the documents.\"\n),\nmodel_path: str = typer.Argument(..., help=\"Folder containing the model.\"),\n):\n\"\"\"\n    Ingest all the text files from `documentation_path` and create a vector store using the Llama model.\n    Args:\n        documentation_path (str): Path to the folder containing the text documents to be ingested.\n        model_path (str): Path to the folder containing the Llama model.\n    Raises:\n        Exception: If no documents are found inside the `documentation_path` folder.\n    Notes:\n        This function creates a new folder with the embedding index in the `index` directory.\n    \"\"\"\nglobal llama\n# initialize the Llama model\nif not llama:\nllama = Llama(model_path=model_path, embedding=True, verbose=False)\nlogger.info(f\"Look for docs in {documentation_path}\")\nchunks = get_sources(documentation_path)\nif not chunks:\nraise Exception(\"No documents were found inside the data folder\")\ncreate_store(chunks, Path(documentation_path).name)\n</code></pre>"},{"location":"query/","title":"Query","text":"<p>This module provides a command line interface for querying a language model using a LLM and a Faiss index of embeddings containing knowledge. It uses the Llama library to interact with the language model and the Faiss library to create and search the index.</p> Note <p>If an index_path is provided, the function loads the Faiss index and searches it for the closest embeddings to the question embedding. It then prompts the user with a message that includes the context of the closest embedding and the original question.</p>"},{"location":"query/#src.query.prompt_with_context","title":"<code>prompt_with_context()</code>","text":"<p>Returns a prompt to request relevant text to answer a given question.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string prompt that includes the given context and question, and asks for relevant text.</p> Source code in <code>src/query.py</code> <pre><code>def prompt_with_context() -&gt; str:\n\"\"\"\n    Returns a prompt to request relevant text to answer a given question.\n    Returns:\n        str: A string prompt that includes the given context and question, and asks for relevant text.\n    \"\"\"\nreturn \"\"\"\n    Use the following portion of a long document to see if any of the text is relevant to answer the question.\n    Context: {context}\n    Question: {question}\n    Provide all relevant text to the question verbatim. If nothing relevant return \"I do not know\" and stop answering.\n    Answer:\"\"\"\n</code></pre>"},{"location":"query/#src.query.query","title":"<code>query(question=typer.Argument(Ellipsis, help='Question to answer.'), model_path=typer.Argument(Ellipsis, help='Folder containing the model.'), index_path=typer.Argument(None, help='Folder containing the vector store with the embeddings. If none provided, only LLM is used.'))</code>","text":"<p>Ask a question to a Language Model (LLM) using an index of embeddings containing the knowledge.</p> <p>If no <code>index_path</code> is specified, it will only use the LLM to answer the question. Otherwise, it will use the embeddings in the <code>index_path</code> to find relevant text before prompting for an answer.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to answer.</p> <code>typer.Argument(Ellipsis, help='Question to answer.')</code> <code>model_path</code> <code>str</code> <p>The folder containing the LLM model.</p> <code>typer.Argument(Ellipsis, help='Folder containing the model.')</code> <code>index_path</code> <code>Optional[Path]</code> <p>The folder containing the vector store with the embeddings. If none provided,</p> <code>typer.Argument(None, help='Folder containing the vector store with the embeddings. If none provided, only LLM is used.')</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The response will be printed to the console.</p> Source code in <code>src/query.py</code> <pre><code>def query(\nquestion: str = typer.Argument(..., help=\"Question to answer.\"),\nmodel_path: str = typer.Argument(..., help=\"Folder containing the model.\"),\nindex_path: Optional[Path] = typer.Argument(\nNone,\nhelp=\"Folder containing the vector store with the embeddings. If \"\n\"none provided, only LLM is used.\",\n),\n):\n\"\"\"\n    Ask a question to a Language Model (LLM) using an index of embeddings containing the knowledge.\n    If no `index_path` is specified, it will only use the LLM to answer the question. Otherwise, it will use the\n    embeddings in the `index_path` to find relevant text before prompting for an answer.\n    Args:\n        question (str): The question to answer.\n        model_path (str): The folder containing the LLM model.\n        index_path (Optional[Path]): The folder containing the vector store with the embeddings. If none provided,\n        only the LLM will be used.\n    Returns:\n        None: The response will be printed to the console.\n    \"\"\"\nglobal llama\nif not llama:\nif not model_path:\nraise Exception(\n\"You need to specify the model path the first time you ask a question\"\n)\nllama = Llama(model_path=model_path, verbose=False, embedding=True)\nif index_path:\nindex = FaissIndex()\nindex.load(index_path)\nembedded_question = np.array([llama.embed(question)])\ncontext = index.search(embedded_question)\nquestion = prompt_with_context().format(context=context[0], question=question)\nresponse = llama(prompt=question)[\"choices\"][0][\"text\"]\nprint(response.split('\"\"\"')[0])\n</code></pre>"},{"location":"reader/","title":"Reader","text":""},{"location":"reader/#src.utils.reader.get_sources","title":"<code>get_sources(documentation_path)</code>","text":"<p>Get the plain text of all the documents in the given folder and split them into smaller chunks.</p> <p>Parameters:</p> Name Type Description Default <code>documentation_path</code> <code>str</code> <p>Path to the directory containing the documents.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of strings: A list of all the documents, split into smaller chunks.</p> Source code in <code>src/utils/reader.py</code> <pre><code>def get_sources(documentation_path: str) -&gt; List[str]:\n\"\"\"\n    Get the plain text of all the documents in the given folder and split them into smaller chunks.\n    Args:\n        documentation_path (str): Path to the directory containing the documents.\n    Returns:\n        List of strings: A list of all the documents, split into smaller chunks.\n    \"\"\"\nps = list(Path(documentation_path).glob(\"**/*.*\"))\nlogger.info(f\"\ud83d\udcd6 {len(ps)} documents were found\")\ndata = []\nsources = []\nfor p in ps:\ntext = get_text(p)\nif text:\ndata.append(text)\nsources.append(p)\n# Split the documents, as needed, into smaller chunks.\n# We do this due to the context limits of the LLMs.\ndocs = []\nfor i, d in enumerate(data):\nsplits = split_text(d)\ndocs.extend(splits)\nreturn docs\n</code></pre>"},{"location":"reader/#src.utils.reader.get_text","title":"<code>get_text(input_file)</code>","text":"<p>Determines the file extension and converts the file to plain text.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>The path to the file to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plain text content of the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the file extension is not supported or the file cannot be read.</p> Source code in <code>src/utils/reader.py</code> <pre><code>def get_text(input_file: Path) -&gt; str:\n\"\"\"\n    Determines the file extension and converts the file to plain text.\n    Args:\n        input_file (Path): The path to the file to convert.\n    Returns:\n        str: The plain text content of the file.\n    Raises:\n        Exception: If the file extension is not supported or the file cannot be read.\n    \"\"\"\next = input_file.suffix.lower()\nif ext == \".pdf\":\nreturn pdf_to_text(input_file)\nelif ext == \".md\":\nwith open(input_file) as f:\nreturn markdown_to_text(f.read())\nelse:\ntry:\nwith open(input_file) as f:\nreturn f.read()\nexcept:\nlogger.info(f\"Skipping {input_file} because it is not a text file\")\nreturn \"\"\n</code></pre>"},{"location":"reader/#src.utils.reader.markdown_to_text","title":"<code>markdown_to_text(text)</code>","text":"<p>Convert a Markdown string to plain text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>str, the Markdown string to be converted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>str, the plain text content of the Markdown string.</p> Source code in <code>src/utils/reader.py</code> <pre><code>def markdown_to_text(text: str) -&gt; str:\n\"\"\"\n    Convert a Markdown string to plain text.\n    Args:\n        text: str, the Markdown string to be converted.\n    Returns:\n        str, the plain text content of the Markdown string.\n    \"\"\"\nhtml = markdown.markdown(text)\nsoup = BeautifulSoup(html, features=\"html.parser\")\nreturn soup.get_text()\n</code></pre>"},{"location":"reader/#src.utils.reader.pdf_to_text","title":"<code>pdf_to_text(input_file)</code>","text":"<p>Convert a PDF file to plain text.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>Path, the path to the input PDF file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>str, the plain text content of the PDF file.</p> Source code in <code>src/utils/reader.py</code> <pre><code>def pdf_to_text(input_file: Path) -&gt; str:\n\"\"\"\n    Convert a PDF file to plain text.\n    Args:\n        input_file: Path, the path to the input PDF file.\n    Returns:\n        str, the plain text content of the PDF file.\n    \"\"\"\nwith open(input_file, \"rb\") as f:\nreader = PyPDF2.PdfFileReader(f)\ntext = \"\"\nfor i in range(reader.getNumPages()):\npage = reader.getPage(i)\ntext += page.extractText()\nreturn text\n</code></pre>"},{"location":"reader/#src.utils.reader.split_text","title":"<code>split_text(text, separator=' ', chunk_size=512)</code>","text":"<p>Splits a text string into chunks of at most chunk_size characters, using the specified separator character (default is space). Returns a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>str, the text string to be split.</p> required <code>separator</code> <code>str</code> <p>str, the character to use as a separator for splitting. Defaults to ' ' (space).</p> <code>' '</code> <code>chunk_size</code> <code>int</code> <p>int, the maximum length of each chunk. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str], a list of string chunks of the original text.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the text cannot be split into chunks of chunk_size characters using the specified separator.</p> Source code in <code>src/utils/reader.py</code> <pre><code>def split_text(text: str, separator: str = \" \", chunk_size: int = 512) -&gt; List[str]:\n\"\"\"\n    Splits a text string into chunks of at most chunk_size characters,\n    using the specified separator character (default is space).\n    Returns a list of strings.\n    Args:\n        text: str, the text string to be split.\n        separator: str, the character to use as a separator for splitting. Defaults to ' ' (space).\n        chunk_size: int, the maximum length of each chunk. Defaults to 512.\n    Returns:\n        List[str], a list of string chunks of the original text.\n    Raises:\n        Exception: if the text cannot be split into chunks of chunk_size characters using the specified separator.\n    \"\"\"\nchunks = []\nstart = 0\nend = 0\nwhile end &lt; len(text):\nend = min(start + chunk_size, len(text))\nif end == len(text):\nchunks.append(text[start:end])\nbreak\nif text[end] == separator:\nchunks.append(text[start:end])\nstart = end + 1\nelif text[start:end].count(separator) == 0:\nchunks.append(text[start:end])\nstart = end\nelse:\nlast_separator = text[start:end].rfind(separator)\nif not last_separator:\nraise Exception(\nf\"This text cannot be split into chunks of {chunk_size} characters \"\nf\"using the selected separator.\"\n)\nchunks.append(text[start : start + last_separator])\nstart += last_separator + 1\nreturn chunks\n</code></pre>"},{"location":"setup/","title":"How to Run","text":"<ul> <li>The first step is to download the model weights in case you do not have them. There are a few torrents floating around as well as some huggingface repositories (e.g https://huggingface.co/nyanko7/LLaMA-7B/). Once you have them, copy them into the models folder. Any of the quantized models that can run in llama.cpp should work: Llama, Alpaca, GPT4All, Vicuna...</li> <li>Install the necessary requirements available in the requirements.lock. The usage of a virtual environment is recommended. <pre><code>python -m venv env\nsource env/bin/activate\npip install -r requirements.lock\n</code></pre></li> </ul>"},{"location":"setup/#cli","title":"CLI","text":"<p>There are two simple CLI applications:</p> <ul> <li> <p>ingest: create an embedding vector store given a folder with a bunch of documents <pre><code>\u276f python -m src.ingest --help\n Usage: python -m src.ingest [OPTIONS] DOCUMENTATION_PATH MODEL_PATH\n Ingest all the markdown files from documentation_path and create a vector store. It will\n create a new folder with the embedding index.\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    documentation_path      TEXT  Folder containing the documents. [default: None]       \u2502\n\u2502                                    [required]                                             \u2502\n\u2502 *    model_path              TEXT  Folder containing the model. [default: None]           \u2502\n\u2502                                    [required]                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> </li> <li> <p>query: make a question to the LLM given the embedding path <pre><code>\u276f python -m src.query --help\n Usage: python -m src.query [OPTIONS] QUESTION MODEL_PATH [INDEX_PATH]\n Ask a question to a LLM using an index of embeddings containing the knowledge. If no\n index_path is specified it will only use the LLM to answer the question.\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    question        TEXT          Question to answer. [default: None] [required]         \u2502\n\u2502 *    model_path      TEXT          Folder containing the model. [default: None]           \u2502\n\u2502                                    [required]                                             \u2502\n\u2502      index_path      [INDEX_PATH]  Folder containing the vector store with the            \u2502\n\u2502                                    embeddings. If none provided, only LLM is used.        \u2502\n\u2502                                    [default: None]                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> </li> </ul>"},{"location":"setup/#api","title":"API","text":"<p>To run the uvicorn server: <pre><code>uvicorn main:app --reload --host 0.0.0.0 --port 8383\n</code></pre> Then you can hit the two endpoints available:</p> <p>Ingest: <pre><code>curl \"http://0.0.0.0:8383/ingest?documentation_path=data&amp;model_path=models/gpt4all/gpt4all-lora-quantized-new.bin\"\n</code></pre> Query: <pre><code>curl -X POST -H \"Content-Type: application/json\" --data '{\n    \"question\":\"who is the best?\",\n    \"index_path\":\"index/datadog_documentation\",\n    \"model_path\":\"models/gpt4all/gpt4all-lora-quantized-new.bin\"}' http://0.0.0.0:8383/query```\n</code></pre> Note that when hitting the query endpoint:</p> <ul> <li>The <code>index_path</code> is not mandatory. If not provided only the LLM will be used to answer the question.</li> <li>The <code>model_path</code> is only mandatory the first request. Once specified, the model will be already loaded in memory, and you will not need to provide a model path. If another <code>model_path</code> is provided a different model will be loaded.</li> </ul>"},{"location":"special_thanks/","title":"Special Thanks","text":""},{"location":"special_thanks/#special-thanks-to","title":"Special thanks to","text":"<ul> <li>llama.cpp: Port of Facebook's LLaMA model in C/C++</li> <li>llama-cpp-python: Python bindings for llama.cpp</li> <li>faiss: A library for efficient similarity search and clustering of dense vectors.</li> </ul>"},{"location":"utils/","title":"Utils","text":"<p>In the case you have the original .pth models you can use the <code>convert_and_quantize.sh</code> script which is going to:</p> <ul> <li>Clone llama.cpp and compile it</li> <li>Convert the model to FP16 and quantize the model to 4-bits</li> </ul>"}]}